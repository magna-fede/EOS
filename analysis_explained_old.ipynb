{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee241b4f",
   "metadata": {},
   "source": [
    "# Analysis Pipeline for An Eye On Semantics study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a159b5",
   "metadata": {},
   "source": [
    "This is the analysis pipeline for the eye tracking study An Eye On Semantics.\n",
    "As we are going to use some functions for analysing the data, please check the comments within each function.\n",
    "If you need more information, feel free to contact me at Federica.Magnabosco@mrc-cbu.cam.ac.uk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff00964d",
   "metadata": {},
   "source": [
    "Let's start!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841d7808",
   "metadata": {},
   "source": [
    "Import relevant stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18e9d5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "070254dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pygazeanalyser.edfreader import read_edf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba220fe",
   "metadata": {},
   "source": [
    "Set the screen size the data was recorded with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50391218",
   "metadata": {},
   "outputs": [],
   "source": [
    "DISPSIZE = (1280, 1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08f96a9",
   "metadata": {},
   "source": [
    "In pixels, x,y=(0,0) is the top-left corner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7abb648",
   "metadata": {},
   "source": [
    "Now, let's specify which trials need to be excluded because of errors during recording.\n",
    "They will be selected inside (normalised)attach_info function.\n",
    "\n",
    "(key=subject_ID : values=trials_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19d7d36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude = {111: [120,121],\n",
    "           128: [240,241],\n",
    "           130: np.concatenate([[240],\n",
    "                                 np.arange(120,133)]).tolist(),\n",
    "           136: [80,81],\n",
    "           141: np.arange(20,39).tolist()} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f5940c",
   "metadata": {},
   "source": [
    "Function to get all blinks that happened for a certain participant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd668b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_blinks(data_edf):\n",
    "    blinks=[]\n",
    "    for i,trial in enumerate(data_edf):\n",
    "        blinks.append(data_edf[i]['events']['Eblk']) # get all blinks\n",
    "        blinks = [x for x in blinks if x != []]\n",
    "    blinks = [item for sublist in blinks for item in sublist]    \n",
    "    return blinks "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e0454c",
   "metadata": {},
   "source": [
    "This function is similar to pygaze analyser read_edf. The main difference is that it does not distinguish between different trials, but instead creates a dataframe for each participant with columns time and event ID. This will be used to find when participants blinked, and exclude trials that happened during blinks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e704a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_edf_plain(filename):\n",
    "    \"\"\"Get a dataframe containing only and all the events from the EDF file,\n",
    "        with the trackertime, not dividing the trials\"\"\"\n",
    "    # check if the file exists\n",
    "    if os.path.isfile(filename):\n",
    "        # open file\n",
    "        f = open(filename, 'r')\n",
    "    # raise exception if the file does not exist\n",
    "    else:\n",
    "        raise Exception(\"Error in read_edf: file '%s' does not exist\" % filename)\n",
    "    raw = f.readlines()\n",
    "    f.close()\n",
    "    # variables\n",
    "    data = []\n",
    "    event = []\n",
    "    timepoint = []\n",
    "    # loop through all lines\n",
    "    for line in raw:\n",
    "        if line[0:4] == \"SFIX\":\n",
    "            l = line[9:]\n",
    "            timepoint.append(int(l))\n",
    "            event.append(line[0:4])\n",
    "        elif line[0:4] == \"EFIX\":\n",
    "            l = line[9:]\n",
    "            l = l.split('\\t')\n",
    "            timepoint.append(int(l[1]))\n",
    "            event.append(line[0:4])\n",
    "         \t\t\t# saccade start\n",
    "        elif line[0:5] == 'SSACC':\n",
    "            l = line[9:]\n",
    "            timepoint.append(int(l))\n",
    "            event.append(line[0:5])\n",
    "         \t\t\t# saccade end\n",
    "        elif line[0:5] == \"ESACC\":\n",
    "            l = line[9:]\n",
    "            l = l.split('\\t')\n",
    "            timepoint.append(int(l[1]))\n",
    "            event.append(line[0:5])\n",
    "         \t\t\t# blink start\n",
    "        elif line[0:6] == \"SBLINK\":\n",
    "            l = line[9:]\n",
    "            timepoint.append(int(l))\n",
    "            event.append(line[0:6])\n",
    "         \t\t\t# blink end\n",
    "        elif line[0:6] == \"EBLINK\":\n",
    "            l = line[9:]\n",
    "            l = l.split('\\t')\n",
    "            timepoint.append(int(l[1]))\n",
    "            event.append(line[0:6])\n",
    "   \t# return\n",
    "    data = pd.DataFrame()\n",
    "    data['time'] = np.array(timepoint)\n",
    "    data['event'] = np.array(event)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8e4a73",
   "metadata": {},
   "source": [
    "This is probably the most important function. In here those fixations that happened within the Area of Interesest (aka, the target word) are selected. Plus, checked that those are valid fixations. We are retrieving some information sent to Eyelink during the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec3c8018",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixAOI(data_edf,data_plain):\n",
    "    \"\"\"Get all fixations within AOI. Checks that are not followed by a regression\n",
    "    after the first fixation within the AOI + trials that do not contain\n",
    "    a blink or error\"\"\"\n",
    "    # get all fixation durations within a certain AOI for all trials for one subject\n",
    "    # dur_all is the list where we include all the fixation durations that\n",
    "    # respect certain inclusion criteria\n",
    "    # \n",
    "    dur_all = []\n",
    "    regressed = []\n",
    "    time_before_fix = []\n",
    "    tot_number_fixation = []\n",
    "    # loop over each trial (remember that data_edf is a list, and each trial is a dict)        \n",
    "    for i,trial in enumerate(data_edf):\n",
    "        # select the trial's events, and select only the END OF FIXATION EVENT ('Efix')\n",
    "        # and save the relative information into a pd.DataFrame\n",
    "        # (specifically Efix is a list of lists, now converted to a pd.DataFrame)\n",
    "        pd_fix = pd.DataFrame.from_records(trial['events']['Efix'],\n",
    "                                           columns=['start',\n",
    "                                                    'end',\n",
    "                                                    'duration',\n",
    "                                                    'x',\n",
    "                                                    'y'])\n",
    "        # save the total number of fixations that happened in a certain trial\n",
    "        tot_number_fixation.append(len(pd_fix))\n",
    "        \n",
    "        # exclude those trials where all fixations are outside the screen\n",
    "        # it used to happen if there was an error in the gaze position detection\n",
    "        # it should not be a problem now, considering that gaze is required\n",
    "        # to trigger the start of the sentence\n",
    "        if (((pd_fix['x'] < 0).all()) or ((pd_fix['x'] > DISPSIZE[0]).all())):\n",
    "            dur_all.append('Error in fixation detection')\n",
    "            time_before_fix.append(np.nan)\n",
    "            regressed.append(np.nan)\n",
    "        elif (((pd_fix['y'] < 0).all()) or ((pd_fix['y'] > DISPSIZE[1]).all())):\n",
    "            dur_all.append('Error in fixation detection')\n",
    "            time_before_fix.append(np.nan)\n",
    "            regressed.append(np.nan)\n",
    "        # or when no fixations have been detected\n",
    "        elif len(pd_fix)<2:\n",
    "            dur_all.append('Error in fixation detection')\n",
    "            time_before_fix.append(np.nan)\n",
    "            regressed.append(np.nan)\n",
    "        # if not all fixations are outside the screen, we'll go ahead    \n",
    "        \n",
    "        else:    \n",
    "            # the following info is gathered from the stimulus presentation software\n",
    "            # (specifically we look at the \"msg\" events, and extract the relevant info)\n",
    "            # we know which messages to look at, because for each trial the messages sent\n",
    "            # are always the same\n",
    "            \n",
    "            # tuple indicating dimension of each sentence in pixels\n",
    "            size = re.search(\"SIZE OF THE STIMULUS: (.*)\\n\",trial['events']['msg'][3][1])\n",
    "            size = eval(size.group(1)) # tuple (width,height)\n",
    "            \n",
    "            # size of each letter in pixels\n",
    "            # this should is identical for each sentence, equal to 11 in our study\n",
    "            unit = re.search(\"NUMBER OF CHARACTERS: (.*)\\n\",trial['events']['msg'][4][1])\n",
    "            unit = size[0]/eval(unit.group(1)) \n",
    "            \n",
    "            # position (in characters) of the target word inside the sentence\n",
    "            pos_target = re.search(\"POS TARGET INSIDE BOX: (.*)\\n\",trial['events']['msg'][5][1])\n",
    "            pos_target = eval(pos_target.group(1))\n",
    "            \n",
    "            # position (in pixels) of the target word\n",
    "            # convert width to the position in x, y cohordinates where the sentence starts\n",
    "            # stimulus starting position is = centre of x_axis screen - half size of the sentence\n",
    "            # because sentence is presented aligned to the centre of the screen\n",
    "            pos_startstim = DISPSIZE[0]/2-size[0]/2\n",
    "\n",
    "            # no need to calculate y as always in the same position at the centre\n",
    "            # there's only one line\n",
    "            \n",
    "            # get x and y position of the target word\n",
    "            # as pos_target is in characters, we need to mutiply each letter*unit\n",
    "            # including in the AOI also half space preceding and half space\n",
    "            # following the target word\n",
    "            # tuple (x0,x1) position of the target word in pixels \n",
    "            target_x = (pos_startstim+(pos_target[0]*unit)-unit/2,pos_startstim+(pos_target[1]*unit)+unit/2)\n",
    "            \n",
    "            # AOI limits for target_y position is two times the height of the letters\n",
    "            # no need to be too strict as there's just one line\n",
    "            target_y = (DISPSIZE[1]/2-size[1]*2,DISPSIZE[1]/2+size[1]*2)\n",
    "            \n",
    "            # get all fixations on target word\n",
    "            # Specifically this gets x position if targetstart_position<fixation_position<targetend_position\n",
    "            # for both x and y \n",
    "            fixAOI = pd_fix['x'][(target_x[0]<pd_fix['x']) &\n",
    "                                 (pd_fix['x']<target_x[1]) &\n",
    "                                 (target_y[0]<pd_fix['y']) &\n",
    "                                 (pd_fix['y']<target_y[1])]\n",
    "            \n",
    "            # check if at least one fixation on target \n",
    "            if len(fixAOI)>0:\n",
    "                \n",
    "                # check this is first pass\n",
    "                # by checking if all previous fixations (indetify by index) have a smaller x_position\n",
    "                # (meaning that this is a first pass fixation, not skipped on a first instance)\n",
    "                if all(pd_fix['x'][0:fixAOI.index[0]]<fixAOI[fixAOI.index[0]]):\n",
    "                \n",
    "                # check if this is not the last fixation (if so, automatically there's no regression)    \n",
    "                    if (len(pd_fix['x'])>(fixAOI.index[0]+1)):\n",
    "                        dur_all.append(pd_fix['duration'][(target_x[0]<pd_fix['x']) &\n",
    "                                                  (pd_fix['x']<target_x[1]) &\n",
    "                                                  (target_y[0]<pd_fix['y']) &\n",
    "                                                  (pd_fix['y']<target_y[1])\n",
    "                                                  ])\n",
    "                        time_before_fix.append(pd_fix['start'][fixAOI.index[0]] - pd_fix['start'][0])\n",
    "                        \n",
    "                    # check if there is a regression to BEFORE the target area\n",
    "                    # and save it in the relevant list \n",
    "                        if (pd_fix['x'].iloc[fixAOI.index[0]+1]>target_x[0]):\n",
    "                        # if there wasn't a regression, save as 0\n",
    "                            regressed.append(0)\n",
    "                        else:\n",
    "                        # if there was a regression, save as 1\n",
    "                            regressed.append(1)\n",
    "                    else:\n",
    "                    # if this is the last fixation, than there is no regression\n",
    "                    # so, get the fixations (otherwise it will give an error\n",
    "                    # when explicitly looking if fixation is followed by regression)\n",
    "                    # however, there should always be a fixation after on the square\n",
    "                        dur_all.append(pd_fix['duration'][(target_x[0]<pd_fix['x']) &\n",
    "                                                  (pd_fix['x']<target_x[1]) &\n",
    "                                                  (target_y[0]<pd_fix['y']) &\n",
    "                                                  (pd_fix['y']<target_y[1])\n",
    "                                                  ])\n",
    "                        # get rest of the data\n",
    "                        time_before_fix.append(pd_fix['start'][fixAOI.index[0]] - pd_fix['start'][0])\n",
    "                        regressed.append(0)\n",
    "                        \n",
    "                # this relates to the fixation being first pass\n",
    "                else:\n",
    "                    dur_all.append('Nope - not fixated during first pass')       \n",
    "                    time_before_fix.append(np.nan)\n",
    "                    regressed.append(np.nan)\n",
    "                    \n",
    "            else:\n",
    "                # if there is no fixation, return empty Series\n",
    "                # by returning empy series, we know that the word has been skipped\n",
    "                dur_all.append(pd_fix['duration'][(target_x[0]<pd_fix['x']) &\n",
    "                                                  (pd_fix['x']<target_x[1]) &\n",
    "                                                  (target_y[0]<pd_fix['y']) &\n",
    "                                                  (pd_fix['y']<target_y[1])\n",
    "                                                  ])\n",
    "                time_before_fix.append(np.nan)\n",
    "                regressed.append(np.nan)\n",
    "                \n",
    "            # now check blinks\n",
    "            # first, check at least one fixation and that the object is not string\n",
    "            # remember that when trials are to be discarded there's a string\n",
    "            \n",
    "            if ((len(dur_all[-1])>0) and (type(dur_all[-1])!=str)):\n",
    "            \n",
    "                # get trackertime of the start of the first fixation on target words\n",
    "                start = pd_fix['start'].iloc[dur_all[-1].index[0]]\n",
    "            \n",
    "                # get the  position in the events only list of data\n",
    "                plain_start = data_plain[data_plain['time']==start].index[0]\n",
    "                r = range(plain_start-2,plain_start+4)\n",
    "                \n",
    "                # this range because each blink generates an artefactual saccade event\n",
    "                # so each blink is surrounded by SSACC and ESAC events\n",
    "                # different ends (i.e., -2, +4) to include also EFIX event\n",
    "                \n",
    "                # this basically checks whether the fixation is immediately\n",
    "                # preceded or followed by a blink\n",
    "                if (any(data_plain['event'].iloc[r]=='SBLINK')\n",
    "                    or any(data_plain['event'].iloc[r]=='EBLINK')):\n",
    "                        dur_all[-1] = 'There was a blink'\n",
    "                        time_before_fix[-1] = np.nan\n",
    "                        regressed[-1] = np.nan\n",
    "                        \n",
    "    # returning:\n",
    "    # dur_all is a list (len = 400) of series,\n",
    "    #     each series contains all the fixations within AOI for that trial\n",
    "    #     each element consist in index = ordinal number of fixation for that trial\n",
    "    #     (eg if the first fixation within AOI was the 6th, index=6)\n",
    "    #      duration = duration of the fixation in ms\n",
    "    # regressed is binary info if that trial was regressed (0=not regressed, 1=regressed)\n",
    "    # time_before_fix is duration in ms from start of trial to first fixation made on AOI\n",
    "    # tot_number_fixation is how many fixation happened in that trial\n",
    "    \n",
    "    return dur_all, regressed, time_before_fix, tot_number_fixation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6871b1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get both FFD and GD\n",
    "def ffdgd(dur_all):\n",
    "    \"\"\"Get first-fixation, gaze duration, whether it was fixated\"\"\"\n",
    "    # dur_all is list of series, output from fixAOI\n",
    "    # set everything to zero\n",
    "    # this is convenient as words skipped have durations = 0 and fixated = 0\n",
    "    FFD = np.zeros(len(dur_all))\n",
    "    GD = np.zeros(len(dur_all))\n",
    "    fixated = np.zeros(len(dur_all))\n",
    "    \n",
    "    # n_prior_fixation is set as nan, so specifying it only if there's a fixation\n",
    "    n_prior_fixations = np.empty((len(dur_all)))\n",
    "    n_prior_fixations[:] = np.nan\n",
    "    \n",
    "    # loop over each trial\n",
    "    for i,trial in enumerate(dur_all):\n",
    "        # if error in fixation, then indicate as NAN\n",
    "        if type(trial)==str: # as all trials that should be excluded are strings ...\n",
    "            if trial == 'Nope - not fixated during first pass':\n",
    "                # ... apart if skipped during first pass and only then fixated\n",
    "                # note that in this case it counts as skipped (not as invalid!)\n",
    "                pass # so it stays to zero (it counts as skipped)\n",
    "            else:\n",
    "                # this will allow us to discard them from the analysis\n",
    "                # when regressions, blinks or errors\n",
    "                FFD[i] = np.nan\n",
    "                GD[i] = np.nan\n",
    "                fixated[i] = np.nan\n",
    "        else:\n",
    "            # check if there is at least one fixation in AOI, otherwise FFD=GD=0                \n",
    "            if len(dur_all[i])>0:\n",
    "                \n",
    "                # check if the FIRST fixation fixation is btween 80-600ms long\n",
    "                if (np.array(dur_all[i])[0]>80 and np.array(dur_all[i])[0]<600):\n",
    "                    FFD[i] = np.array(dur_all[i])[0]\n",
    "                    GD[i] = np.array(dur_all[i])[0]\n",
    "                    fixated[i] = 1\n",
    "                # if fixation is longer than 600ms, remove it\n",
    "                elif np.array(dur_all[i])[0]>=600:\n",
    "                    FFD[i] = np.nan\n",
    "                    GD[i] = np.nan\n",
    "                    fixated[i] = 1\n",
    "                # check id there's more than one fixation\n",
    "                # remember dur_all is a list of series, so dur_all[i] is a series with all fixations in that trial\n",
    "                # (given that the first is first pass)\n",
    "                if len(dur_all[i])>1:\n",
    "                    # if more than one, check whether they are consecutive\n",
    "                    # fixations inside the AOI by checking the index\n",
    "                    for j in range(len(dur_all[i].index)-1):\n",
    "                        if ((dur_all[i].index[j+1]-dur_all[i].index[j]==1) &\n",
    "                            (FFD[i]>0)):\n",
    "                            GD[i] += np.array(dur_all[i])[j+1]\n",
    "                        else:\n",
    "                            # this break is needed to get out of the loop as soon\n",
    "                            # as two fixations are not consecutive\n",
    "                            break\n",
    "                n_prior_fixations[i] = dur_all[i].index[0]\n",
    "    # returns numpy arrays (fixated is binary)\n",
    "    return FFD, GD, fixated, n_prior_fixations\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b91fa7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attach_info(eyedata, regressed, time_before_ff, tot_number_fixation, n_prior_fix):\n",
    "    \"\"\"Include single word and sentence level statistics from relevant file\"\"\"\n",
    "    # here will be saved each participant data (so it's list of dataframes)\n",
    "    eyedata_all = []\n",
    "    \n",
    "    # loop over participants\n",
    "    for i,participantdata in enumerate(eyedata):\n",
    "        # this is the log file\n",
    "        stimuli = pd.read_csv(f\"{base_dir}/{participant[i]}/{participant[i]}.txt\",\n",
    "                              header=0,sep='\\t',\n",
    "                              encoding='ISO-8859-1')        \n",
    "        # include info about order of presentation for each trial to FFD/GD\n",
    "        eye_all_i = pd.DataFrame(list(zip(eyedata[i], stimuli.trialnr)),\n",
    "                                 index=stimuli.IDstim,\n",
    "                                 columns=['ms','trialnr'])\n",
    "        # and append other info from eye tracker\n",
    "        eye_all_i['time_before_ff'] = time_before_ff[i]\n",
    "        eye_all_i['regressed'] = regressed[i]\n",
    "        eye_all_i['n_tot_fix'] = tot_number_fixation[i]\n",
    "        eye_all_i['n_prior_fix'] = n_prior_fix[i]\n",
    "        \n",
    "        # check if need to exclude any trial\n",
    "        if participant[i] in exclude:\n",
    "            for tr_number in exclude[participant[i]]:\n",
    "                eye_all_i.drop(eye_all_i[eye_all_i['trialnr']==tr_number].index,\n",
    "                               inplace=True)\n",
    "        \n",
    "        # merge data from eye tracker and predictors\n",
    "        a = pd.merge(eye_all_i, stimuliALL[['ID',\n",
    "                                     'ConcM',\n",
    "                                     'LEN', \n",
    "                                     'UN2_F', \n",
    "                                     'UN3_F', \n",
    "                                     'Orth', \n",
    "                                     'OLD20',\n",
    "                                     'FreqCount', \n",
    "                                     'LogFreq(Zipf)', \n",
    "                                     'V_MeanSum',\n",
    "                                     'A_MeanSum', \n",
    "                                     'mink3_SM', \n",
    "                                     'BLP_rt',\n",
    "                                     'BLP_accuracy', \n",
    "                                     'similarity', \n",
    "                                     'Position',\t\n",
    "                                     'PRECEDING_Frequency',\t\n",
    "                                     'PRECEDING_LogFreq(Zipf)',\t\n",
    "                                     'LENprec',\n",
    "                                     'cloze',\n",
    "                                     'Sim',\n",
    "                                     'plausibility'\n",
    "                                     #'SemD', # when include SemD, you loose 4 trials (don't have SemD for them)\n",
    "                                     #'AoA'\n",
    "                                     ]], how='inner',left_on=['IDstim'],\n",
    "                                         right_on=['ID'])\n",
    "        # append participant data\n",
    "        eyedata_all.append(a)\n",
    "        # remove na\n",
    "        eyedata_all[-1] = eyedata_all[-1][eyedata_all[-1].iloc[:,0].notna()]    \n",
    "    return eyedata_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3020d6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attach_mean_centred(eyedata,regressed, time_before_ff, tot_number_fixation, n_prior_fix):\n",
    "    \"\"\"Supply the participants gd/ffd to obtain a gd/ffd_all that is mean_centred.\n",
    "    Will do the sane as attach info, just for normalised predictors.\"\"\"\n",
    "    norm_eyedata_all = []\n",
    "    for i,participantdata in enumerate(eyedata):\n",
    "        stimuli = pd.read_csv(f\"{base_dir}/{participant[i]}/{participant[i]}.txt\",\n",
    "                              header=0, sep='\\t', encoding='ISO-8859-1')\n",
    "        normalized_all_i = pd.DataFrame(list(zip(participantdata,\n",
    "                                                 stimuli.trialnr)),\n",
    "                                        index=stimuli.IDstim,\n",
    "                                        columns=['ms','trialnr'])\n",
    "        normalized_all_i['time_before_ff'] = time_before_ff[i]\n",
    "        normalized_all_i['regressed'] = regressed[i]\n",
    "        normalized_all_i['n_tot_fix'] = tot_number_fixation[i]\n",
    "        normalized_all_i['n_prior_fix'] = n_prior_fix[i]\n",
    "        normalized_all_i['time_before_ff'] = (normalized_all_i['time_before_ff'] - \\\n",
    "                                              normalized_all_i['time_before_ff'].mean() \\\n",
    "                                                  ) / normalized_all_i['time_before_ff'].std()\n",
    "        normalized_all_i['n_tot_fix'] = (normalized_all_i['n_tot_fix'] - \\\n",
    "                                              normalized_all_i['n_tot_fix'].mean() \\\n",
    "                                                  ) / normalized_all_i['n_tot_fix'].std()\n",
    "        normalized_all_i['n_prior_fix'] = (normalized_all_i['n_prior_fix'] - \\\n",
    "                                              normalized_all_i['n_prior_fix'].mean() \\\n",
    "                                                  ) / normalized_all_i['n_prior_fix'].std()            \n",
    "\n",
    "            \n",
    "        # check if need to exclude any trial\n",
    "        if participant[i] in exclude:\n",
    "            for tr_number in exclude[participant[i]]:\n",
    "                normalized_all_i.drop(normalized_all_i[normalized_all_i['trialnr']==tr_number].index,\n",
    "                               inplace=True)\n",
    "        \n",
    "        # get predictors\n",
    "        normalized_all_i = pd.merge(normalized_all_i,\n",
    "                                    stimuliALL_norm,\n",
    "                                    how='inner',\n",
    "                                    left_on=['IDstim'],\n",
    "                                    right_on=['ID'])\n",
    "        \n",
    "        norm_eyedata_all.append(normalized_all_i)\n",
    "        norm_eyedata_all[-1] = norm_eyedata_all[-1][norm_eyedata_all[-1].iloc[:,0].notna()]\n",
    "    return norm_eyedata_all\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa51d4c",
   "metadata": {},
   "source": [
    "Function are finished, this gets the work done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9d3d2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DISPSIZE = (1280, 1024)\n",
    "# Add information about target word\n",
    "path = \"C:/Users/fm02/OwnCloud/Sentences/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1807d5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(path)\n",
    "# stimuliALL = pd.read_excel('stimuli_all_onewordsemsim.xlsx', engine='openpyxl')\n",
    "stimuliALL = pd.read_excel('stimuli_all_onewordsemsim.xlsx', engine='openpyxl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32e4c76",
   "metadata": {},
   "source": [
    "Normalise predictors (this is needed for lme4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1c9c984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# include only numeric predictors\n",
    "to_norm = stimuliALL[['ConcM',\n",
    "                      'LEN',\n",
    "                      'UN2_F',\n",
    "                      'UN3_F',\n",
    "                      'Orth',\n",
    "                      'OLD20',\n",
    "                      'FreqCount',\n",
    "                      'LogFreq(Zipf)', \n",
    "                     'V_MeanSum',\n",
    "                     'A_MeanSum',\n",
    "                     'mink3_SM',\n",
    "                     'BLP_rt',\n",
    "                     'BLP_accuracy',\n",
    "                     'similarity',\n",
    "                     'Position',\n",
    "                     'PRECEDING_Frequency',\n",
    "                     'PRECEDING_LogFreq(Zipf)',\t\n",
    "                     'LENprec',\n",
    "                     'Predictability',\n",
    "                     'cloze',\n",
    "                     'plausibility',\n",
    "                     'Sim'\n",
    "                     #'SemD', # when include SemD, you loose 4 trials (don't have SemD for them)\n",
    "                     #'AoA'\n",
    "                     ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c167a106",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_norm = (to_norm-to_norm.mean())/to_norm.std()\n",
    "# put back Word and ID\n",
    "stimuliALL_norm = stimuliALL[['Word','ID']].join(to_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2139e7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data from the participants\n",
    "base_dir = \"//cbsu/data/Imaging/hauk/users/fm02/EOS_data/EOS_data_fromLab\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e10d071e",
   "metadata": {},
   "outputs": [],
   "source": [
    "participant = [\n",
    "        101, \n",
    "        102, \n",
    "        103, \n",
    "        104, \n",
    "        105,\n",
    "        106,\n",
    "        107,\n",
    "        108,\n",
    "        109,\n",
    "        110,\n",
    "        111,\n",
    "        112,\n",
    "        113,\n",
    "        114,\n",
    "        115,\n",
    "        116,\n",
    "        117,\n",
    "        118,\n",
    "        119,\n",
    "        120,\n",
    "        121,\n",
    "        122,\n",
    "        123,\n",
    "        124,\n",
    "        125,\n",
    "        126,\n",
    "        127,\n",
    "        128,\n",
    "        129,\n",
    "        130,\n",
    "        131,\n",
    "        132,\n",
    "        133,\n",
    "        134,\n",
    "        135,\n",
    "        136,\n",
    "        137,\n",
    "        138,\n",
    "#        139 # excluded - not completed testing\n",
    "        140,\n",
    "        141\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd468ccb",
   "metadata": {},
   "source": [
    "Initialise dicts where to store participants data (key is participant, value is data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c75b9dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "data_plain = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4d5f4f",
   "metadata": {},
   "source": [
    "This loops over each participant to import the relevant data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bc927f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in participant:\n",
    "    print(f'Reading EDF data participant {i}')\n",
    "    data[i] = read_edf(f\"{base_dir}/{i}/{i}.asc\",\n",
    "                       \"STIMONSET\",\"STIMOFFSET\")\n",
    "    data_plain[i] = read_edf_plain(f\"{base_dir}/{i}/{i}.asc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f69a04",
   "metadata": {},
   "source": [
    "Initialise the list where things will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "952a00f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dur = []\n",
    "regressed = []\n",
    "time_before = []\n",
    "nfix = []\n",
    "\n",
    "ffd = []\n",
    "gd = []\n",
    "prfix = []\n",
    "nprior_fixs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c6c89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over participants  \n",
    "for subject in data.keys():\n",
    "    print(f'Extracting data participant {subject}')\n",
    "    # this basically extracts fixations within AOI\n",
    "    dur_i, regressed_i, time_before_i, nfix_i = fixAOI(data[subject],\n",
    "                                                        data_plain[subject])\n",
    "    # this saves relevant info from relevant fixations\n",
    "    FFD_i, GD_i, fixated_i, nprior_fixs_i = ffdgd(dur_i)\n",
    "    \n",
    "    # append data for each subject\n",
    "    dur.append(dur_i)\n",
    "    regressed.append(regressed_i)\n",
    "    time_before.append(time_before_i)\n",
    "    nfix.append(nfix_i)\n",
    "    \n",
    "    ffd.append(FFD_i)\n",
    "    gd.append(GD_i)\n",
    "    prfix.append(fixated_i)\n",
    "    nprior_fixs.append(nprior_fixs_i)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59e4731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge data and non-normalised predictors\n",
    "gd_all = attach_info(gd, regressed, time_before, nfix, nprior_fixs)\n",
    "ffd_all = attach_info(ffd, regressed, time_before, nfix, nprior_fixs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9391734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge data and normalised predictors\n",
    "norm_gd_all = attach_mean_centred(gd, regressed, time_before, nfix, nprior_fixs)\n",
    "norm_ffd_all = attach_mean_centred(ffd, regressed, time_before, nfix, nprior_fixs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6271c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this retrieves additional information about participant, save in demographi info\n",
    "pis = pd.read_excel(\"//cbsu/data/Imaging/hauk/users/fm02/EOS_data/Demographic_info.xlsx\",\n",
    "                    usecols=[\"Participant ID\",\n",
    "                             \"Gender\",\t\n",
    "                             \"Age\",\t\n",
    "                             \"Handedness\",\t\n",
    "                             \"% Correct Responses\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a16a612",
   "metadata": {},
   "source": [
    "The final bit takes care of transforming the data in long format (which is necessary for fitting Linear Mixed Effect model om R)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0832d36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,df in enumerate(norm_ffd_all):\n",
    "    norm_ffd_all[i] = norm_ffd_all[i].rename(columns={'LogFreq(Zipf)':'LogFreqZipf',\n",
    "                                                    'PRECEDING_LogFreq(Zipf)':'PRECEDING_LogFreqZipf'})\n",
    "    norm_ffd_all[i]['Subject'] = [i]*len(norm_ffd_all[i])\n",
    "    norm_ffd_all[i]['Gender'] = [pis[\"Gender\"][pis[\"Participant ID\"] == participant[i]].values[0]] \\\n",
    "                                        *len(norm_ffd_all[i])\n",
    "    norm_ffd_all[i]['Age'] = [pis[\"Age\"][pis[\"Participant ID\"] == participant[i]].values[0]] \\\n",
    "                                        *len(norm_ffd_all[i])\n",
    "      \n",
    "# GD  - no regressions and normalised predictors, which is probably what we will use   \n",
    "\n",
    "for i,df, in enumerate(norm_gd_all):\n",
    "    norm_gd_all[i] = norm_gd_all[i].rename(columns={'LogFreq(Zipf)':'LogFreqZipf',\n",
    "                                                    'PRECEDING_LogFreq(Zipf)':'PRECEDING_LogFreqZipf'})\n",
    "    norm_gd_all[i]['Subject'] = [i]*len(norm_gd_all[i])\n",
    "    norm_gd_all[i]['Gender'] = [pis[\"Gender\"][pis[\"Participant ID\"] == participant[i]].values[0]] \\\n",
    "                                        *len(norm_gd_all[i])\n",
    "    norm_gd_all[i]['Age'] = [pis[\"Age\"][pis[\"Participant ID\"] == participant[i]].values[0]] \\\n",
    "                                        *len(norm_gd_all[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9699d5",
   "metadata": {},
   "source": [
    "Let's run one example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2ae4e170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading EDF data participant 101\n"
     ]
    }
   ],
   "source": [
    "data = {}\n",
    "data_plain = {}\n",
    "i = 101\n",
    "print(f'Reading EDF data participant {i}')\n",
    "data[i] = read_edf(f\"{base_dir}/{i}/{i}.asc\",\n",
    "                   \"STIMONSET\",\"STIMOFFSET\")\n",
    "data_plain[i] = read_edf_plain(f\"{base_dir}/{i}/{i}.asc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1bd9888f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dur = []\n",
    "regressed = []\n",
    "time_before = []\n",
    "nfix = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c3797e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ffd = []\n",
    "gd = []\n",
    "prfix = []\n",
    "nprior_fixs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6288fae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data participant 101\n"
     ]
    }
   ],
   "source": [
    "for subject in data.keys():\n",
    "    print(f'Extracting data participant {subject}')\n",
    "    dur_i, regressed_i, time_before_i, nfix_i = fixAOI(data[subject],\n",
    "                                                        data_plain[subject])\n",
    "    \n",
    "    FFD_i, GD_i, fixated_i, nprior_fixs_i = ffdgd(dur_i)\n",
    "    \n",
    "    dur.append(dur_i)\n",
    "    regressed.append(regressed_i)\n",
    "    time_before.append(time_before_i)\n",
    "    nfix.append(nfix_i)\n",
    "    \n",
    "    ffd.append(FFD_i)\n",
    "    gd.append(GD_i)\n",
    "    prfix.append(fixated_i)\n",
    "    nprior_fixs.append(nprior_fixs_i)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aba53608",
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_all = attach_info(gd, regressed, time_before, nfix, nprior_fixs)\n",
    "ffd_all = attach_info(ffd, regressed, time_before, nfix, nprior_fixs)\n",
    "norm_gd_all = attach_mean_centred(gd, regressed, time_before, nfix, nprior_fixs)\n",
    "norm_ffd_all = attach_mean_centred(ffd, regressed, time_before, nfix, nprior_fixs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6daf9d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "pis = pd.read_excel(\"//cbsu/data/Imaging/hauk/users/fm02/EOS_data/Demographic_info.xlsx\",\n",
    "                    usecols=[\"Participant ID\",\n",
    "                             \"Gender\",\t\n",
    "                             \"Age\",\t\n",
    "                             \"Handedness\",\t\n",
    "                             \"% Correct Responses\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "57766de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fa0e62",
   "metadata": {},
   "source": [
    "Let's explore the first trial of the first participant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "34cdadd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial = data[101][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0ef32003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0ca1a692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['x', 'y', 'size', 'time', 'trackertime', 'events'])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5eb59f75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Sfix', 'Ssac', 'Sblk', 'Efix', 'Esac', 'Eblk', 'msg'])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial['events'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7248a23b",
   "metadata": {},
   "source": [
    "Save 'Efix' events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "edc6023e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_fix = pd.DataFrame.from_records(trial['events']['Efix'],\n",
    "                                   columns=['start',\n",
    "                                            'end',\n",
    "                                            'duration',\n",
    "                                            'x',\n",
    "                                            'y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f09bb4",
   "metadata": {},
   "source": [
    "We get information from all the fixations that happened during that trial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "73f8fa20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>duration</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1455422</td>\n",
       "      <td>1455802</td>\n",
       "      <td>381</td>\n",
       "      <td>72.0</td>\n",
       "      <td>513.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1455837</td>\n",
       "      <td>1456107</td>\n",
       "      <td>271</td>\n",
       "      <td>343.4</td>\n",
       "      <td>506.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1456117</td>\n",
       "      <td>1456255</td>\n",
       "      <td>139</td>\n",
       "      <td>323.4</td>\n",
       "      <td>506.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1456280</td>\n",
       "      <td>1456452</td>\n",
       "      <td>173</td>\n",
       "      <td>416.3</td>\n",
       "      <td>505.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1456475</td>\n",
       "      <td>1456640</td>\n",
       "      <td>166</td>\n",
       "      <td>503.1</td>\n",
       "      <td>502.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1456664</td>\n",
       "      <td>1456865</td>\n",
       "      <td>202</td>\n",
       "      <td>593.6</td>\n",
       "      <td>497.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1456886</td>\n",
       "      <td>1457021</td>\n",
       "      <td>136</td>\n",
       "      <td>649.2</td>\n",
       "      <td>500.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1457049</td>\n",
       "      <td>1457225</td>\n",
       "      <td>177</td>\n",
       "      <td>765.6</td>\n",
       "      <td>507.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1457253</td>\n",
       "      <td>1457417</td>\n",
       "      <td>165</td>\n",
       "      <td>876.1</td>\n",
       "      <td>510.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1457435</td>\n",
       "      <td>1457684</td>\n",
       "      <td>250</td>\n",
       "      <td>933.0</td>\n",
       "      <td>512.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1457704</td>\n",
       "      <td>1458380</td>\n",
       "      <td>677</td>\n",
       "      <td>978.3</td>\n",
       "      <td>505.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1459117</td>\n",
       "      <td>1459271</td>\n",
       "      <td>155</td>\n",
       "      <td>382.3</td>\n",
       "      <td>535.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1459301</td>\n",
       "      <td>1459707</td>\n",
       "      <td>407</td>\n",
       "      <td>328.5</td>\n",
       "      <td>516.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1459745</td>\n",
       "      <td>1459963</td>\n",
       "      <td>219</td>\n",
       "      <td>624.0</td>\n",
       "      <td>543.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1459994</td>\n",
       "      <td>1460132</td>\n",
       "      <td>139</td>\n",
       "      <td>770.2</td>\n",
       "      <td>520.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1460168</td>\n",
       "      <td>1460349</td>\n",
       "      <td>182</td>\n",
       "      <td>976.3</td>\n",
       "      <td>511.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      start      end  duration      x      y\n",
       "0   1455422  1455802       381   72.0  513.3\n",
       "1   1455837  1456107       271  343.4  506.0\n",
       "2   1456117  1456255       139  323.4  506.1\n",
       "3   1456280  1456452       173  416.3  505.4\n",
       "4   1456475  1456640       166  503.1  502.3\n",
       "5   1456664  1456865       202  593.6  497.3\n",
       "6   1456886  1457021       136  649.2  500.2\n",
       "7   1457049  1457225       177  765.6  507.5\n",
       "8   1457253  1457417       165  876.1  510.3\n",
       "9   1457435  1457684       250  933.0  512.1\n",
       "10  1457704  1458380       677  978.3  505.0\n",
       "11  1459117  1459271       155  382.3  535.5\n",
       "12  1459301  1459707       407  328.5  516.3\n",
       "13  1459745  1459963       219  624.0  543.2\n",
       "14  1459994  1460132       139  770.2  520.8\n",
       "15  1460168  1460349       182  976.3  511.3"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "437c879c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The stimulus dimensions, in pixels, is (803, 23), respectively x and y axes\n"
     ]
    }
   ],
   "source": [
    "size = re.search(\"SIZE OF THE STIMULUS: (.*)\\n\",trial['events']['msg'][3][1])\n",
    "size = eval(size.group(1))\n",
    "print(f\"The stimulus dimensions, in pixels, is {size}, respectively x and y axes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "36889ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each character occupies 11.0 pixels\n"
     ]
    }
   ],
   "source": [
    "unit = re.search(\"NUMBER OF CHARACTERS: (.*)\\n\",trial['events']['msg'][4][1])\n",
    "unit = size[0]/eval(unit.group(1))\n",
    "print(f\"Each character occupies {unit} pixels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "166ce81c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position of the target word within the sentence, from character 37 to character 44\n"
     ]
    }
   ],
   "source": [
    "pos_target = re.search(\"POS TARGET INSIDE BOX: (.*)\\n\",trial['events']['msg'][5][1])\n",
    "pos_target = eval(pos_target.group(1))\n",
    "print(f\"Position of the target word within the sentence, from character {pos_target[0]} to character {pos_target[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "be5e2cb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "238.5"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_startstim = DISPSIZE[0]/2-size[0]/2\n",
    "pos_startstim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a666d99e",
   "metadata": {},
   "source": [
    "This is the location of the target word, in pixels (x_start, x_end) (y_start, y_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4f285b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(640.0, 728.0) (466.0, 558.0)\n"
     ]
    }
   ],
   "source": [
    "target_x = (pos_startstim+(pos_target[0]*unit)-unit/2,pos_startstim+(pos_target[1]*unit)+unit/2)\n",
    "target_y = (DISPSIZE[1]/2-size[1]*2,DISPSIZE[1]/2+size[1]*2)\n",
    "print(target_x, target_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5eac61e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixAOI = pd_fix['x'][(target_x[0]<pd_fix['x']) &\n",
    "                                      (pd_fix['x']<target_x[1]) &\n",
    "                                      (target_y[0]<pd_fix['y']) &\n",
    "                                      (pd_fix['y']<target_y[1])]\n",
    "durAOI = pd_fix['duration'][(target_x[0]<pd_fix['x']) &\n",
    "                                      (pd_fix['x']<target_x[1]) &\n",
    "                                      (target_y[0]<pd_fix['y']) &\n",
    "                                      (pd_fix['y']<target_y[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2012f2e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6    649.2\n",
       "Name: x, dtype: float64"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fixAOI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c9e1f601",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6    136\n",
       "Name: duration, dtype: int64"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "durAOI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "bfc75cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 7th fixation was on the target word and lasted 136 ms\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(fixAOI)):\n",
    "    print(f\"The {fixAOI.index[i]+1}th fixation was on the target word and lasted {durAOI.values[i]} ms\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
